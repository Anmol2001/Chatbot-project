{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/programming_questions.txt\",\"r\",encoding=\"utf-8\") as file:\n",
    "    que=file.read()\n",
    "with open(\"data/programming_answers.txt\",\"r\",encoding=\"utf-8\") as file:\n",
    "    ans=file.read()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "que_sentences=que.split('\\n')\n",
    "facts_sentences=ans.split('\\n')\n",
    "print(len(que_sentences),len(facts_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nlp=spacy.load(\"en\",disable=[\"parser\",\"ner\"])\n",
    "lem=WordNetLemmatizer\n",
    "def pre_process(facts):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    # Tokenlization\n",
    "    facts=[x.lower() for x in facts]\n",
    "    facts = [re.sub('[/(){}\\[\\]\\|@,;]', ' ', x) for x in facts]\n",
    "    facts = [re.sub('[^0-9a-z #+_]', '', x) for x in facts]\n",
    "    \n",
    "    facts_tokens = [nltk.word_tokenize(t) for t in facts]\n",
    "    # Removing Stop Words\n",
    "    facts_stop = [[t for t in tokens if (t not in stop_words) and (2 <= len(t.strip()) < 25)]\n",
    "                      for tokens in facts_tokens]\n",
    "    facts_lem=[[t.lemma_ for t in nlp(\" \".join(tokens)) if t.text!=\"-PRON-\"] for tokens in facts_stop]\n",
    "    facts_lem=pd.Series(facts_lem)\n",
    "    return facts_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['give', 'information', 'internship']\n",
      "40\n",
      "40 40 40\n"
     ]
    }
   ],
   "source": [
    "facts_pp=pre_process(facts_sentences)\n",
    "que_pp=pre_process(que_sentences)\n",
    "print(pre_process([\"give some information about internship\" ])[0])\n",
    "print(len(que_sentences))\n",
    "print(len(que_pp),len(facts_sentences),len(facts_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facts</th>\n",
       "      <th>Facts_Tokens</th>\n",
       "      <th>Que</th>\n",
       "      <th>Que_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>try this: https://code.visualstudio.com/docs/c...</td>\n",
       "      <td>[try, https, codevisualstudiocom, docs, cpp, c...</td>\n",
       "      <td>How to install g++ correctly for VS Code?</td>\n",
       "      <td>[install, g++, correctly, vs, code]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anyone having language doubts for now you can ...</td>\n",
       "      <td>[anyone, language, doubt, continue, python, al...</td>\n",
       "      <td>I started learning c++, should I discontinue i...</td>\n",
       "      <td>[start, learn, c++, discontinue, learn, python...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Completely you can never learn a language 100%...</td>\n",
       "      <td>[completely, never, learn, language, 100, ever...</td>\n",
       "      <td>In how much time we can learn a language, like...</td>\n",
       "      <td>[much, time, learn, language, like, python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 3rd sem, there'd be this course ADP.It cove...</td>\n",
       "      <td>[3rd, sem, there, d, course, adpit, cover, htm...</td>\n",
       "      <td>will we be taught js later on?</td>\n",
       "      <td>[teach, js, later]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since the Data Science courses require Python,...</td>\n",
       "      <td>[since, data, science, course, require, python...</td>\n",
       "      <td>if i am already a little accustomed with pytho...</td>\n",
       "      <td>[already, little, accustomed, python, java, li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Facts  \\\n",
       "0  try this: https://code.visualstudio.com/docs/c...   \n",
       "1  Anyone having language doubts for now you can ...   \n",
       "2  Completely you can never learn a language 100%...   \n",
       "3  In 3rd sem, there'd be this course ADP.It cove...   \n",
       "4  Since the Data Science courses require Python,...   \n",
       "\n",
       "                                        Facts_Tokens  \\\n",
       "0  [try, https, codevisualstudiocom, docs, cpp, c...   \n",
       "1  [anyone, language, doubt, continue, python, al...   \n",
       "2  [completely, never, learn, language, 100, ever...   \n",
       "3  [3rd, sem, there, d, course, adpit, cover, htm...   \n",
       "4  [since, data, science, course, require, python...   \n",
       "\n",
       "                                                 Que  \\\n",
       "0          How to install g++ correctly for VS Code?   \n",
       "1  I started learning c++, should I discontinue i...   \n",
       "2  In how much time we can learn a language, like...   \n",
       "3                     will we be taught js later on?   \n",
       "4  if i am already a little accustomed with pytho...   \n",
       "\n",
       "                                          Que_Tokens  \n",
       "0                [install, g++, correctly, vs, code]  \n",
       "1  [start, learn, c++, discontinue, learn, python...  \n",
       "2        [much, time, learn, language, like, python]  \n",
       "3                                 [teach, js, later]  \n",
       "4  [already, little, accustomed, python, java, li...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens = pd.DataFrame({'Facts': facts_sentences,\n",
    "                            'Facts_Tokens': facts_pp,\n",
    "                            'Que': que_sentences,\n",
    "                            'Que_Tokens': que_pp,\n",
    "                           })\n",
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "def train_model(train_data):\n",
    "    \"\"\"Function trains and creates Word2vec Model using parsed\n",
    "    data and returns trained model\"\"\"\n",
    "    #model = gensim.models.KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    #model.train(train_data)\n",
    "    #model = gensim.models.Word2Vec(train_data, min_count=1,size=150)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_tokens['Fact_Vectors'] = None\n",
    "data_tokens['Average_Pooling'] = None\n",
    "\n",
    "facts_data = list(data_tokens['Facts_Tokens'])\n",
    "que_data=list(data_tokens['Que_Tokens'])\n",
    "# Train model\n",
    "model_name = 'word2vec_model_.wordvectors' \n",
    "#model = train_model(facts_data)\n",
    "#trained_model.save(model_name)\n",
    "#print('Saved %s model successfully' % model_name)\n",
    "local_model= gensim.models.Word2Vec(facts_data+que_data, min_count=1,size=300)\n",
    "local_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin',\n",
    "                                #lockf=1.0,\n",
    "                            binary=True)\n",
    "\"\"\"# Save Word2Vec model\n",
    "word2vec_pickle_path =  'college_queries_word2vec_' + '.bin'\n",
    "f = open(word2vec_pickle_path, 'wb')\n",
    "pickle.dump(trained_model, f) \n",
    "f.close()\n",
    "    \n",
    "model = gensim.models.KeyedVectors.load(word2vec_pickle_path)\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the vectors for each question\n",
    "#print(len(data_tokens))\n",
    "#local_model= gensim.models.Word2Vec(facts_data, min_count=1,size=300)\n",
    "#local_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin',\n",
    "#                                lockf=1.0,\n",
    " #                               binary=True)\n",
    "local_model.train(que_data+facts_data,total_examples=local_model.corpus_count, epochs=15)\n",
    "data_tokens['Que_Vectors'] = None\n",
    "for i in range(len(data_tokens)):\n",
    "            que_tokens = data_tokens['Que_Tokens'][i]\n",
    "            que_vectors = []\n",
    "            for token in que_tokens:\n",
    "                try:\n",
    "                    vector = local_model[token]\n",
    "                    que_vectors.append(vector)\n",
    "                except:\n",
    "                    continue\n",
    "            # Vectors for each tokens\n",
    "            data_tokens['Que_Vectors'][i] = que_vectors\n",
    "            #print(len(data_tokens['Fact_Vectors'][i]))\n",
    "            # Average Pooling of all tokens\n",
    "            data_tokens['Average_Pooling'][i] = list(pd.DataFrame(que_vectors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Word2Vec model\n",
    "word2vec_pickle_path =  'college_queries_word2vec_' + '.bin'\n",
    "f = open(word2vec_pickle_path, 'wb')\n",
    "pickle.dump(local_model, f) \n",
    "f.close()\n",
    "    \n",
    "#model = gensim.models.KeyedVectors.load(word2vec_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facts</th>\n",
       "      <th>Facts_Tokens</th>\n",
       "      <th>Que</th>\n",
       "      <th>Que_Tokens</th>\n",
       "      <th>Average_Pooling</th>\n",
       "      <th>Que_Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The total area of IIT Mandi is 538 acres of wh...</td>\n",
       "      <td>[total, area, iit, mandi, 538, acre, 200, acre...</td>\n",
       "      <td>What is the area of IIT Mandi?</td>\n",
       "      <td>[area, iit, mandi]</td>\n",
       "      <td>[-0.010416666666666666, 0.054768880208333336, ...</td>\n",
       "      <td>[[-0.072265625, 0.052490234, 0.053955078, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On 23rd September 2012, 108 B.Tech 2nd year st...</td>\n",
       "      <td>[23rd, september, 2012, 108, btech, 2nd, year,...</td>\n",
       "      <td>When did students shift to main campus of IIT ...</td>\n",
       "      <td>[student, shift, main, campus, iit, mandi]</td>\n",
       "      <td>[-0.026285807291666668, 0.007181803385416667, ...</td>\n",
       "      <td>[[0.036865234, 0.020141602, 0.22167969, 0.1552...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The main campus of IIT Mandi is in Kamand vall...</td>\n",
       "      <td>[main, campus, iit, mandi, kamand, valley, vil...</td>\n",
       "      <td>What is the location of IIT Mandi?</td>\n",
       "      <td>[location, iit, mandi]</td>\n",
       "      <td>[0.024576822916666668, 0.005208333333333333, -...</td>\n",
       "      <td>[[0.032714844, -0.096191406, 0.044189453, 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is great variation in the climatic condi...</td>\n",
       "      <td>[great, variation, climatic, condition, himach...</td>\n",
       "      <td>How is the weather at IIT Mandi?</td>\n",
       "      <td>[weather, iit, mandi]</td>\n",
       "      <td>[-0.048502604166666664, 0.09326171875, -0.1297...</td>\n",
       "      <td>[[-0.18652344, 0.16796875, -0.30273438, -0.055...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These regions enjoys a wet-sub temperate clima...</td>\n",
       "      <td>[region, enjoy, wetsub, temperate, climate, fo...</td>\n",
       "      <td>What is the geography of Mandi?</td>\n",
       "      <td>[geography, mandi]</td>\n",
       "      <td>[0.203125, 0.09226226806640625, -0.06604003906...</td>\n",
       "      <td>[[0.13671875, -0.0019989014, 0.033935547, 0.29...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Facts  \\\n",
       "0  The total area of IIT Mandi is 538 acres of wh...   \n",
       "1  On 23rd September 2012, 108 B.Tech 2nd year st...   \n",
       "2  The main campus of IIT Mandi is in Kamand vall...   \n",
       "3  There is great variation in the climatic condi...   \n",
       "4  These regions enjoys a wet-sub temperate clima...   \n",
       "\n",
       "                                        Facts_Tokens  \\\n",
       "0  [total, area, iit, mandi, 538, acre, 200, acre...   \n",
       "1  [23rd, september, 2012, 108, btech, 2nd, year,...   \n",
       "2  [main, campus, iit, mandi, kamand, valley, vil...   \n",
       "3  [great, variation, climatic, condition, himach...   \n",
       "4  [region, enjoy, wetsub, temperate, climate, fo...   \n",
       "\n",
       "                                                 Que  \\\n",
       "0                     What is the area of IIT Mandi?   \n",
       "1  When did students shift to main campus of IIT ...   \n",
       "2                 What is the location of IIT Mandi?   \n",
       "3                   How is the weather at IIT Mandi?   \n",
       "4                    What is the geography of Mandi?   \n",
       "\n",
       "                                   Que_Tokens  \\\n",
       "0                          [area, iit, mandi]   \n",
       "1  [student, shift, main, campus, iit, mandi]   \n",
       "2                      [location, iit, mandi]   \n",
       "3                       [weather, iit, mandi]   \n",
       "4                          [geography, mandi]   \n",
       "\n",
       "                                     Average_Pooling  \\\n",
       "0  [-0.010416666666666666, 0.054768880208333336, ...   \n",
       "1  [-0.026285807291666668, 0.007181803385416667, ...   \n",
       "2  [0.024576822916666668, 0.005208333333333333, -...   \n",
       "3  [-0.048502604166666664, 0.09326171875, -0.1297...   \n",
       "4  [0.203125, 0.09226226806640625, -0.06604003906...   \n",
       "\n",
       "                                         Que_Vectors  \n",
       "0  [[-0.072265625, 0.052490234, 0.053955078, -0.0...  \n",
       "1  [[0.036865234, 0.020141602, 0.22167969, 0.1552...  \n",
       "2  [[0.032714844, -0.096191406, 0.044189453, 0.17...  \n",
       "3  [[-0.18652344, 0.16796875, -0.30273438, -0.055...  \n",
       "4  [[0.13671875, -0.0019989014, 0.033935547, 0.29...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as data as JSON\n",
    "data_json = json.loads(data_tokens.to_json(orient='records'))\n",
    "\n",
    "with open( 'college_query.json', 'w') as outfile:\n",
    "    json.dump(data_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Que</th>\n",
       "      <th>Facts</th>\n",
       "      <th>Facts_Tokens</th>\n",
       "      <th>Que_Vectors</th>\n",
       "      <th>Que_Tokens</th>\n",
       "      <th>Average_Pooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>what is ubuntu?</td>\n",
       "      <td>Ubuntu is a Linux-based operating system. It i...</td>\n",
       "      <td>[ubuntu, linuxbase, operating, system, design,...</td>\n",
       "      <td>[[0.34765625, 0.142578125, 0.251953125, 0.2714...</td>\n",
       "      <td>[ubuntu]</td>\n",
       "      <td>[0.34765625, 0.142578125, 0.251953125, 0.27148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Do I need to use Ubuntu or I can use Windows?</td>\n",
       "      <td>There is no requirement to use ubuntu, but it ...</td>\n",
       "      <td>[requirement, use, ubuntu, preffere, use, ubun...</td>\n",
       "      <td>[[0.0072631836, 0.1005859375, -0.0600585938, 0...</td>\n",
       "      <td>[need, use, ubuntu, use, window]</td>\n",
       "      <td>[0.1358276367, 0.0135742188, 0.049609375, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Can I have both Windows and Ubuntu installed o...</td>\n",
       "      <td>Yes u can dual-boot Ubuntu and windows on a sa...</td>\n",
       "      <td>[yes, dualboot, ubuntu, window, machine, link,...</td>\n",
       "      <td>[[0.1953125, -0.0563964844, 0.0146484375, -0.0...</td>\n",
       "      <td>[windows, ubuntu, instal, computer]</td>\n",
       "      <td>[0.0903320312, 0.0220336914, 0.1245117188, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>How to dual-boot Ubuntu and Windows?</td>\n",
       "      <td>Here is a link https://www.tecmint.com/install...</td>\n",
       "      <td>[link, https, wwwtecmintcom, still, problem, d...</td>\n",
       "      <td>[[-0.0044202921, 0.0017646761, 0.0155917592, 0...</td>\n",
       "      <td>[dualboot, ubuntu, window]</td>\n",
       "      <td>[0.1472895901, 0.0769228608, 0.0633027218, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Where can i find a learning guide for Ubuntu?</td>\n",
       "      <td>Check this documentation for Ubuntu https://ub...</td>\n",
       "      <td>[check, documentation, ubuntu, https, ubuntuco...</td>\n",
       "      <td>[[-0.0069580078, -0.0437011719, -0.16796875, 0...</td>\n",
       "      <td>[find, learn, guide, ubuntu]</td>\n",
       "      <td>[0.0704650879, 0.0267028809, -0.0515136719, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Que  \\\n",
       "165                                    what is ubuntu?   \n",
       "166      Do I need to use Ubuntu or I can use Windows?   \n",
       "167  Can I have both Windows and Ubuntu installed o...   \n",
       "168               How to dual-boot Ubuntu and Windows?   \n",
       "169      Where can i find a learning guide for Ubuntu?   \n",
       "\n",
       "                                                 Facts  \\\n",
       "165  Ubuntu is a Linux-based operating system. It i...   \n",
       "166  There is no requirement to use ubuntu, but it ...   \n",
       "167  Yes u can dual-boot Ubuntu and windows on a sa...   \n",
       "168  Here is a link https://www.tecmint.com/install...   \n",
       "169  Check this documentation for Ubuntu https://ub...   \n",
       "\n",
       "                                          Facts_Tokens  \\\n",
       "165  [ubuntu, linuxbase, operating, system, design,...   \n",
       "166  [requirement, use, ubuntu, preffere, use, ubun...   \n",
       "167  [yes, dualboot, ubuntu, window, machine, link,...   \n",
       "168  [link, https, wwwtecmintcom, still, problem, d...   \n",
       "169  [check, documentation, ubuntu, https, ubuntuco...   \n",
       "\n",
       "                                           Que_Vectors  \\\n",
       "165  [[0.34765625, 0.142578125, 0.251953125, 0.2714...   \n",
       "166  [[0.0072631836, 0.1005859375, -0.0600585938, 0...   \n",
       "167  [[0.1953125, -0.0563964844, 0.0146484375, -0.0...   \n",
       "168  [[-0.0044202921, 0.0017646761, 0.0155917592, 0...   \n",
       "169  [[-0.0069580078, -0.0437011719, -0.16796875, 0...   \n",
       "\n",
       "                              Que_Tokens  \\\n",
       "165                             [ubuntu]   \n",
       "166     [need, use, ubuntu, use, window]   \n",
       "167  [windows, ubuntu, instal, computer]   \n",
       "168           [dualboot, ubuntu, window]   \n",
       "169         [find, learn, guide, ubuntu]   \n",
       "\n",
       "                                       Average_Pooling  \n",
       "165  [0.34765625, 0.142578125, 0.251953125, 0.27148...  \n",
       "166  [0.1358276367, 0.0135742188, 0.049609375, 0.09...  \n",
       "167  [0.0903320312, 0.0220336914, 0.1245117188, 0.1...  \n",
       "168  [0.1472895901, 0.0769228608, 0.0633027218, 0.0...  \n",
       "169  [0.0704650879, 0.0267028809, -0.0515136719, 0....  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    stackoverflow_path = 'college_query.json'\n",
    "\n",
    "    with open(stackoverflow_path) as file:\n",
    "        reader = json.load(file)\n",
    "        facts = []\n",
    "        facts_tokens = []\n",
    "        que=[]\n",
    "        que_tokens = []\n",
    "        que_vectors = []\n",
    "        average_pooling = []\n",
    "        for row in reader:\n",
    "            facts.append(row['Facts'])\n",
    "            facts_tokens.append(row['Facts_Tokens'])\n",
    "            que_vectors.append(row['Que_Vectors'])\n",
    "            average_pooling.append(row['Average_Pooling'])\n",
    "            que.append(row['Que'])\n",
    "            que_tokens.append(row['Que_Tokens'])\n",
    "\n",
    "        data_tokens = pd.DataFrame({'Que':que,\n",
    "                                    'Facts': facts,\n",
    "                                    'Facts_Tokens': facts_tokens,\n",
    "                                    'Que_Vectors': que_vectors,\n",
    "                                    'Que_Tokens': que_tokens,\n",
    "                                    'Average_Pooling': average_pooling})\n",
    "\n",
    "data_tokens.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Talk_To_Jim(data, model,sentence):\n",
    "    \n",
    "    # Preprocessing of user input\n",
    "    sentence_pp = pre_process([sentence])[0]\n",
    "    cosines = []\n",
    "    try:\n",
    "        # Get vectors and average pooling\n",
    "        fact_vectors = []\n",
    "        for token in sentence_pp:\n",
    "            try:\n",
    "                vector = model[token]\n",
    "                #print(vector.flags)\n",
    "                if token=='iit' or token=='mandi':\n",
    "                    vector.setflags(write=1)\n",
    "                    vector/=5\n",
    "                fact_vectors.append(vector)\n",
    "                print(\"y\")\n",
    "            except:\n",
    "                #vector = model[token]\n",
    "                #fact_vectors.append(vector)\n",
    "                #print(\"gy\")\n",
    "                #print(token,\"not in vocab\")\n",
    "                continue\n",
    "        #print(len(fact_vectors[0]),len(sentence_pp))\n",
    "        fact_ap = list(pd.DataFrame(fact_vectors).mean())\n",
    "        #print(len(fact_ap))\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        for t in data['Average_Pooling']:\n",
    "            if t is not None and len(t) == len(fact_ap):\n",
    "                #print(\"cs\")\n",
    "                val = cosine_similarity([fact_ap], [t])\n",
    "                cosines.append(val[0][0])\n",
    "            else:\n",
    "                cosines.append(0)\n",
    "    #except:\n",
    "     #   pass\n",
    "        #print(*[x for x in cosines if x>=0.63])        \n",
    "    \n",
    "        #else: \n",
    "        # Sort similarity\n",
    "        index_s =[]\n",
    "        score_s = []\n",
    "        for i in range(len(cosines)):\n",
    "            x = cosines[i]\n",
    "            if x >= 0.65:\n",
    "                #print(\"s\")\n",
    "                index_s.append(i)\n",
    "                score_s.append(cosines[i])\n",
    "\n",
    "        reply_indexes = pd.DataFrame({'index': index_s, 'score': score_s})\n",
    "        reply_indexes = reply_indexes.sort_values(by=\"score\" , ascending=False)\n",
    "        \n",
    "        \n",
    "        # Find Top Facts and Score\n",
    "        r_index = int(reply_indexes['index'].iloc[0])\n",
    "        r_score = float(reply_indexes['score'].iloc[0])\n",
    "        reply = str(data.iloc[:,1][r_index])\n",
    "    except:\n",
    "            return \"Please refer GCS facebook page or ask you mentor for more info :)\",999\n",
    "\n",
    "    \n",
    "        \n",
    "    return reply, r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "y\n",
      "Semester exchange offered by IIT Mandi allows students to take one or two semester(s) from the institutions with whom IIT Mandi has an MoU for such visits. 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "reply,score=Talk_To_Jim(data_tokens,local_model,\"What is semester exchange? \")\n",
    "print(reply,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56144077"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity([\"vegetable\"],[\"fruit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similar_by_word('hello', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#doc2vec \\nfrom gensim.test.utils import common_texts\\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\\ndocuments = [TaggedDocument(data_tokens[\\'Facts_Tokens\\'][i], [i]) for i in range(len(data_tokens))]\\ndoc_model = Doc2Vec(documents, vector_size=150, window=10, min_count=1, workers=4)\\n\\nfrom gensim.test.utils import get_tmpfile\\nfname = get_tmpfile(\"doc2vec_model\")\\ndoc_model.save(fname)\\ndoc_model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\\n\\ndef talk_to_doc(data,model,sentence):\\n    # Preprocessing of user input\\n    sentence_pp = pre_process([sentence])[0]\\n    scores=[]\\n    indexes=[]\\n    for i in range(len(data)):\\n        scores.append(model.n_similarity(sentence_pp,data[\\'Facts_Tokens\\'][i]))\\n        indexes.append(i)\\n    reply_indexes = pd.DataFrame({\\'index\\': indexes, \\'score\\': scores})\\n    reply_indexes = reply_indexes.sort_values(by=\"score\" , ascending=False)\\n    print(*scores)\\n    # Find Top Facts and Score\\n    r_index = int(reply_indexes[\\'index\\'].iloc[0])\\n    r_score = float(reply_indexes[\\'score\\'].iloc[0])\\n\\n    reply = str(data.iloc[:,0][r_index])\\n        \\n    return reply, r_score\\n        \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#doc2vec \n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(data_tokens['Facts_Tokens'][i], [i]) for i in range(len(data_tokens))]\n",
    "doc_model = Doc2Vec(documents, vector_size=150, window=10, min_count=1, workers=4)\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"doc2vec_model\")\n",
    "doc_model.save(fname)\n",
    "doc_model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n",
    "\n",
    "def talk_to_doc(data,model,sentence):\n",
    "    # Preprocessing of user input\n",
    "    sentence_pp = pre_process([sentence])[0]\n",
    "    scores=[]\n",
    "    indexes=[]\n",
    "    for i in range(len(data)):\n",
    "        scores.append(model.n_similarity(sentence_pp,data['Facts_Tokens'][i]))\n",
    "        indexes.append(i)\n",
    "    reply_indexes = pd.DataFrame({'index': indexes, 'score': scores})\n",
    "    reply_indexes = reply_indexes.sort_values(by=\"score\" , ascending=False)\n",
    "    print(*scores)\n",
    "    # Find Top Facts and Score\n",
    "    r_index = int(reply_indexes['index'].iloc[0])\n",
    "    r_score = float(reply_indexes['score'].iloc[0])\n",
    "\n",
    "    reply = str(data.iloc[:,0][r_index])\n",
    "        \n",
    "    return reply, r_score\n",
    "        \n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iit', 'mandi', 'located']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process([\"Where is IIT Mandi located?\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
